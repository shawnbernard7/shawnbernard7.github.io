{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44526825",
   "metadata": {},
   "source": [
    "# Pubmed Metadata ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3e92f",
   "metadata": {},
   "source": [
    "In this exercise, I will demonstrate extracting article metadata from PubMed articles. Pubmed articles are publically available as XMLs here: https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_bulk/. There are daily updates and this page should contain all articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb4ada0c-0215-4efa-b3fc-a3fc09d8217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## required imports\n",
    "import requests\n",
    "import bs4\n",
    "import gzip\n",
    "import shutil\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "import tarfile\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0e3b0-95f6-4ee5-92a7-cb64770849eb",
   "metadata": {},
   "source": [
    "I will use the requests library to pull data directly from the site. I'll go into the oa_comm folder (commercially available articles) and into the xml folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d806d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\\n<html>\\n <head>\\n  <title>Index of /pub/pmc/oa_bulk/oa_comm/xml</title>\\n </head>\\n <body>\\n<h1>Index of /pub/pmc/oa_bulk/oa_comm/xml</h1>\\n<pre>Name                                                      Last modified      Size  <hr><a href=\"/pub/pmc/oa_bulk/oa_comm/\">Parent Directory</a>                                                               -   \\n<a href=\"oa_comm_xml.PMC000xxxxxx.baseline.2023-12-18.filelist.csv\">oa_comm_xml.PMC000xxxxxx.baseline.2023-12-18.filelist.csv</a> 2023-12-18 01:15  335K  \\n<a href=\"oa_comm_xml.PMC000xxxxxx.baseline.2023-12-18.filelist.txt\">oa_comm_xml.PMC000xxxxxx.baseline.2023-12-18.filelist.txt</a> 2023-12-18 01:15  318K  \\n<a href=\"oa_comm_xml.PMC000xxxxxx.baseline.2023-12-18.tar.gz\">oa_comm_xml.PMC000xxxxxx.baseline.2023-12-18.tar.gz</a>       2023-12-18 01:16   42M  \\n<a href=\"oa_comm_xml.PMC001xxxxxx.baseline.2023-12-18.filelist.csv\">oa_comm_xml.PMC001xxxxxx.baseline.2023-12-18.filelist.csv</a> 2023-12-18 01:16  3.1M  \\n<a href=\"oa_comm_xml.PMC001xxxxxx.baseline.2023-12-18.filelist.txt\">oa_comm_xml.PMC001xxxxxx.baseline.2023-12-18.filelist.txt</a> 2023-12-18 01:16  3.0M  \\n<a href=\"oa_comm_xml.PMC001xxxxxx.baseline.2023-12-18.tar.gz\">oa_comm_xml.PMC001xxxxxx.baseline.2023-12-18.tar.gz</a>       2023-12-18 01:21  384M  \\n<a href=\"oa_comm_xml.PMC002xxxxxx.baseline.2023-12-18.filelist.csv\">oa_comm_xml.PMC002xxxxxx.baseline.2023-12-18.filelist.csv</a> 2023-12-18 01:21   14M  \\n<a href=\"oa_comm_xml.PMC002xxxxxx.baseline.2023-12-18.filelist.txt\">oa_comm_xml.PMC002xxxxxx.baseline.2023-12-18.filelist.txt</a> 2023-12-18 01:21   14M  \\n<a href=\"oa_comm_xml.PMC002xxxxxx.baseline.2023-12-18.tar.gz\">oa_comm_xml.PMC002xxxxxx.baseline.2023-12-18.tar.gz</a>       2023-12-18 01:42  1.6G  \\n<a href=\"oa_comm_xml.PMC003xxxxxx.baseline.2023-12-18.filelist.csv\">oa_comm_xml.PMC003xxxxxx.baseline.2023-12-18.filelist.csv</a> 2023-12-18 01:42   38M  \\n<a href=\"oa_comm_xml.PMC003xxxxxx.baseline.2023-12-18.filelist'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_bulk/oa_comm/xml/\"\n",
    "r = requests.get(url) ## pull data from specified url\n",
    "r.text[0:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c3dc13",
   "metadata": {},
   "source": [
    "The request returns the oa_comm/xml url's html file, and you can see that the files under this path are listed with html tag 'a'. In order to extract these file names, I will use the BeautifulSoup library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97933e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/pub/pmc/oa_bulk/oa_comm/\">Parent Directory</a>,\n",
       " <a href=\"oa_comm_xml.PMC000xxxxxx.baseline.2023-12-18.filelist.csv\">oa_comm_xml.PMC000xxxxxx.baseline.2023-12-18.filelist.csv</a>,\n",
       " <a href=\"oa_comm_xml.PMC000xxxxxx.baseline.2023-12-18.filelist.txt\">oa_comm_xml.PMC000xxxxxx.baseline.2023-12-18.filelist.txt</a>,\n",
       " <a href=\"oa_comm_xml.PMC000xxxxxx.baseline.2023-12-18.tar.gz\">oa_comm_xml.PMC000xxxxxx.baseline.2023-12-18.tar.gz</a>,\n",
       " <a href=\"oa_comm_xml.PMC001xxxxxx.baseline.2023-12-18.filelist.csv\">oa_comm_xml.PMC001xxxxxx.baseline.2023-12-18.filelist.csv</a>,\n",
       " <a href=\"oa_comm_xml.PMC001xxxxxx.baseline.2023-12-18.filelist.txt\">oa_comm_xml.PMC001xxxxxx.baseline.2023-12-18.filelist.txt</a>,\n",
       " <a href=\"oa_comm_xml.PMC001xxxxxx.baseline.2023-12-18.tar.gz\">oa_comm_xml.PMC001xxxxxx.baseline.2023-12-18.tar.gz</a>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = bs4.BeautifulSoup(r.text, \"html.parser\")\n",
    "files = data.find_all('a') ## get all instances with tag 'a'\n",
    "files[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf711c0e",
   "metadata": {},
   "source": [
    "When extracting elements with an html tag of 'a', we yield only the filenames under this directory (other metadata/formatting removed). As you can see, for each folder of xmls (PMC000, PMC001), there is a filelist in csv and txt format, which contains file metadata. The article XMLs are located inside of the .tar.gz files (compressed folders). I will select only the .tar.gz filenames prior to iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "918e3994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oa_comm_xml.PMC000xxxxxx.baseline.2023-12-18.tar.gz',\n",
       " 'oa_comm_xml.PMC001xxxxxx.baseline.2023-12-18.tar.gz',\n",
       " 'oa_comm_xml.PMC002xxxxxx.baseline.2023-12-18.tar.gz',\n",
       " 'oa_comm_xml.PMC003xxxxxx.baseline.2023-12-18.tar.gz',\n",
       " 'oa_comm_xml.PMC004xxxxxx.baseline.2023-12-18.tar.gz']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_gz_files = [x.text for x in files if str.endswith(x.text, 'tar.gz')] ## .text to remove html tags\n",
    "tar_gz_files[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f480ed8c",
   "metadata": {},
   "source": [
    "First, I will show the file retrieval process of just one XML from the first .tar.gz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd06f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<!DOCTYPE article PUBLIC \"-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN\" \"JATS-archivearticle1.dtd\">\\n<article article-type=\"research-article\" xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><?properties open_access?><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">PLoS Biol</journal-id><journal-id journal-id-type=\"iso-abbrev\">PLoS Biol</journal-id><journal-id journal-id-type=\"publisher-id\">pbio</journal-id><journal-id journal-id-type=\"pmc\">plosbiol</journal-id><journal-title-group><journal-title>PLoS Biology</journal-title></journal-title-group><issn pub-type=\"ppub\">1544-9173</issn><issn pub-type=\"epub\">1545-7885</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmid\">12929205</article-id><article-id pub-id-type=\"pmc\">PMC176545</article-id><article-id pub-id-type=\"doi\">10.1371/journal.pbio.0000005</article-id><article-categories><subj-group subj-group-type=\"heading\"><subject>Research Article</subject></subj-group><subj-group subj-group-type=\"Discipline\"><subject>Genetics/Genomics/Gene Therapy</subject><subject>Infectious Diseases</subject><subject>Microbiology</subject></subj-group><subj-group subj-group-type=\"System Taxonomy\"><subject>Plasmodium</subject></subj-group></article-categories><title-group><article-title>The Transcriptome of the Intraerythrocytic Developmental Cycle of <named-content content-type=\"genus-species\">Plasmodium falciparum</named-content>\\n</article-title><alt-title alt-title-type=\"running-head\"><named-content content-type=\"genus-species\">P. falciparum</named-content> IDC Transcriptome</alt-title></title-group><contrib-group><contrib contrib-type=\"author\" equal-contrib=\"yes\"><name><surname>Bozdech</surname><given-names>Zbynek</given-names></name><xref ref-type=\"aff\" rid=\"aff1\">\\n<sup>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for l in tar_gz_files[0:1]: ## first .gz file\n",
    "    tar_gz = requests.get(url + l) ## get .tar.gz file from site (url specified above)\n",
    "    with tarfile.open(fileobj=BytesIO(tar_gz.content)) as xmls: ## decompress tar.gz file, extract metadata\n",
    "        for xml in xmls.getnames()[0:1]: ## first xml for demonstration here\n",
    "            xmls.extract(xml) ## locally save single xml\n",
    "            with open(xml, \"rb\") as f: ## read contents of locally saved xml\n",
    "                Bs_data = BeautifulSoup(f, \"xml\")\n",
    "str(Bs_data)[0:2000] ## show head of xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faceddba",
   "metadata": {},
   "source": [
    "As you can see, these XMLs have tags as well. Lets check how these tags are distributed across articles so we can identify the metadata we can extract. I will iterate across the first 100 xmls and yield tag counts for each. The XMLs will be extracted using the process shown above, but I will add one tweak to the process: I will delete the locally saved XML after the metadata is parsed in the iterations. This will limit the local storage used during this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ff3dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_tag_distribs = [] ## empty list to store parsed xml data for all articles\n",
    "for l in tar_gz_files[0:1]: ## first .gz file\n",
    "    tar_gz = requests.get(url + l) ## get .tar.gz file from site (url specified above)\n",
    "    with tarfile.open(fileobj=BytesIO(tar_gz.content)) as xmls: ## decompress tar.gz file, extract metadata\n",
    "        for xml in xmls.getnames()[0:100]: ## first 100 xmls\n",
    "            xmls.extract(xml) ## locally save single xml\n",
    "            with open(xml, \"rb\") as f: ## read contents of locally saved xml\n",
    "                Bs_data = BeautifulSoup(f, \"xml\") ## save xml contents into beautifulsoup\n",
    "                tag_counts = pd.Series([tag.name for tag in Bs_data.find_all()]).value_counts() ## get counts of tags\n",
    "                tag_counts = tag_counts.reset_index().rename(columns = {0:xml+'_count', 'index':'tag'}) ## format tag counts into pd df\n",
    "                xml_tag_distribs.append(tag_counts)\n",
    "            os.remove(xml) ## remove file from local instance\n",
    "            os.rmdir(str.split(xml, '/')[0]) ## remove created folder from local instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff7356bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>PMC000xxxxxx/PMC176545.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC176546.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC176547.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC176548.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC193604.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC193605.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC193606.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC193607.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC212319.xml_count</th>\n",
       "      <th>...</th>\n",
       "      <th>PMC000xxxxxx/PMC340961.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC340962.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC340963.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC340964.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC350664.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC350667.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC350672.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC350674.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC359389.xml_count</th>\n",
       "      <th>PMC000xxxxxx/PMC368155.xml_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>given-names</td>\n",
       "      <td>269.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>surname</td>\n",
       "      <td>269.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>name</td>\n",
       "      <td>269.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tag  PMC000xxxxxx/PMC176545.xml_count  \\\n",
       "0  given-names                             269.0   \n",
       "1      surname                             269.0   \n",
       "2         name                             269.0   \n",
       "\n",
       "   PMC000xxxxxx/PMC176546.xml_count  PMC000xxxxxx/PMC176547.xml_count  \\\n",
       "0                              96.0                               NaN   \n",
       "1                              96.0                               NaN   \n",
       "2                              96.0                               NaN   \n",
       "\n",
       "   PMC000xxxxxx/PMC176548.xml_count  PMC000xxxxxx/PMC193604.xml_count  \\\n",
       "0                               NaN                             160.0   \n",
       "1                               NaN                             160.0   \n",
       "2                               NaN                             160.0   \n",
       "\n",
       "   PMC000xxxxxx/PMC193605.xml_count  PMC000xxxxxx/PMC193606.xml_count  \\\n",
       "0                             209.0                               NaN   \n",
       "1                             209.0                               NaN   \n",
       "2                             209.0                               NaN   \n",
       "\n",
       "   PMC000xxxxxx/PMC193607.xml_count  PMC000xxxxxx/PMC212319.xml_count  ...  \\\n",
       "0                               NaN                             215.0  ...   \n",
       "1                               NaN                             215.0  ...   \n",
       "2                               NaN                             215.0  ...   \n",
       "\n",
       "   PMC000xxxxxx/PMC340961.xml_count  PMC000xxxxxx/PMC340962.xml_count  \\\n",
       "0                               NaN                               NaN   \n",
       "1                               NaN                               NaN   \n",
       "2                               NaN                               NaN   \n",
       "\n",
       "   PMC000xxxxxx/PMC340963.xml_count  PMC000xxxxxx/PMC340964.xml_count  \\\n",
       "0                               1.0                               NaN   \n",
       "1                               1.0                               NaN   \n",
       "2                               1.0                               NaN   \n",
       "\n",
       "   PMC000xxxxxx/PMC350664.xml_count  PMC000xxxxxx/PMC350667.xml_count  \\\n",
       "0                             320.0                             303.0   \n",
       "1                             320.0                             303.0   \n",
       "2                             320.0                             303.0   \n",
       "\n",
       "   PMC000xxxxxx/PMC350672.xml_count  PMC000xxxxxx/PMC350674.xml_count  \\\n",
       "0                               NaN                               NaN   \n",
       "1                               NaN                               NaN   \n",
       "2                               NaN                               NaN   \n",
       "\n",
       "   PMC000xxxxxx/PMC359389.xml_count  PMC000xxxxxx/PMC368155.xml_count  \n",
       "0                               2.0                              19.0  \n",
       "1                               2.0                              19.0  \n",
       "2                               2.0                              19.0  \n",
       "\n",
       "[3 rows x 101 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_tag_counts = reduce(lambda  left,right: pd.merge(left,right,on=['tag'],\n",
    "                                            how='outer'), xml_tag_distribs)\n",
    "article_tag_counts.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a210b3",
   "metadata": {},
   "source": [
    "We have extracted a dataframe with rows unique by XML tag, and tag counts for each XML. I will check for tags which are present in all 100 XMLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb4c5b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p',\n",
       " 'year',\n",
       " 'article-title',\n",
       " 'volume',\n",
       " 'subject',\n",
       " 'month',\n",
       " 'day',\n",
       " 'journal-id',\n",
       " 'pub-date',\n",
       " 'article-id',\n",
       " 'subj-group',\n",
       " 'issn',\n",
       " 'publisher-name',\n",
       " 'publisher-loc',\n",
       " 'article',\n",
       " 'body',\n",
       " 'license-p',\n",
       " 'journal-meta',\n",
       " 'journal-title-group',\n",
       " 'journal-title',\n",
       " 'publisher',\n",
       " 'article-meta',\n",
       " 'article-categories',\n",
       " 'title-group',\n",
       " 'issue',\n",
       " 'elocation-id',\n",
       " 'front',\n",
       " 'permissions',\n",
       " 'copyright-statement',\n",
       " 'license']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_tag_counts.dropna()['tag'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27469a89",
   "metadata": {},
   "source": [
    "These are attributes which we can extract for all 100 XMLs. There can be value in looking for more sparse tags if a specific attribute is desired. For this demo, I'll keep it simple. Let's extract article title, journal title, article id, and publication date. I'll first view all occurences of these tags in the last article we iterated over so we can see if there are additional subtypes of these tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dd91e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<article-title>Identifying Protein Function—A Call for Community Action</article-title>,\n",
       " <article-title>Life with 6000 genes</article-title>,\n",
       " <article-title>The <italic>hemK</italic> gene in <named-content content-type=\"genus-species\">Escherichia coli</named-content> encodes the N5-glutamine methyltransferase that modifies peptide release factors</article-title>,\n",
       " <article-title>HemK, a class of protein methyl transferase with similarity to DNA methyl transferases, methylates polypeptide chain release factors, and <italic>hemK</italic> knockout induces defects in translational termination</article-title>,\n",
       " <article-title>Cloning and sequencing of a previously unidentified gene that is involved in the biosynthesis of heme in <named-content content-type=\"genus-species\">Escherichia coli</named-content>\n",
       " </article-title>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bs_data.find_all('article-title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f343162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<journal-title>PLoS Biology</journal-title>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bs_data.find_all('journal-title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e3aa388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<article-id pub-id-type=\"pmid\">15024411</article-id>,\n",
       " <article-id pub-id-type=\"pmc\">PMC368155</article-id>,\n",
       " <article-id pub-id-type=\"doi\">10.1371/journal.pbio.0020042</article-id>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bs_data.find_all('article-id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "665964fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pub-date pub-type=\"ppub\"><month>3</month><year>2004</year></pub-date>,\n",
       " <pub-date pub-type=\"epub\"><day>16</day><month>3</month><year>2004</year></pub-date>,\n",
       " <pub-date pub-type=\"pmc-release\"><day>16</day><month>3</month><year>2004</year></pub-date>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bs_data.find_all('pub-date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9659b102",
   "metadata": {},
   "source": [
    "We see that article-id and pub-date tags have subtypes. In parsing, I will ensure that we are extracting PMC for article-id, and epub (electronic publication date) for pub-date. Now, I'll create a function to exract the desired metadata from an XML read into BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1350623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(Bs_data):\n",
    "    ## get article title\n",
    "    if Bs_data.find('article-title'): ## these if statements are there because .getText raises an error if tag isn't found\n",
    "        article_title = Bs_data.find('article-title').getText()\n",
    "    else:\n",
    "        article_title = None\n",
    "    ## get journal title\n",
    "    if Bs_data.find('journal-title'):\n",
    "        journal_title = Bs_data.find('journal-title').getText()\n",
    "    else:\n",
    "        journal_title = None\n",
    "    ## get pmc\n",
    "    if Bs_data.find('article-id', {'pub-id-type' : \"pmc\"}):\n",
    "        pmc = Bs_data.find('article-id', {'pub-id-type' : \"pmc\"}).getText()\n",
    "    else:\n",
    "        pmc = None\n",
    "    ## get epub date\n",
    "    epub_date = Bs_data.find('pub-date', {'pub-type' : 'epub'})\n",
    "    if (epub_date is not None and epub_date.find('day') and epub_date.find('month') and epub_date.find('year')):\n",
    "        epub_day = epub_date.find('day').getText()\n",
    "        epub_month = epub_date.find('month').getText()\n",
    "        epub_year = epub_date.find('year').getText()\n",
    "        epub_date = epub_month + '/' + epub_day + '/' + epub_year ## concat date to yield common format\n",
    "    else:\n",
    "        epub_date = None\n",
    "    xml_metadata = {'pmc':pmc, 'article_title':article_title, 'journal_title':journal_title, 'epub_date':epub_date}\n",
    "    return xml_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c4f6fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmc': 'PMC368155',\n",
       " 'article_title': 'Identifying Protein Function—A Call for Community Action',\n",
       " 'journal_title': 'PLoS Biology',\n",
       " 'epub_date': '3/16/2004'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_xml(Bs_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea1a117",
   "metadata": {},
   "source": [
    "The function successfully extracted PMC, article and journal title, and electronic publication date from the XML data for the article. Next, I will iterate over the first 100 xmls in the first .tar.gz file and extract this metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95e5183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data = [] ## empty list to store parsed xml data for all articles\n",
    "for l in tar_gz_files[0:1]: ## first .gz file\n",
    "    tar_gz = requests.get(url + l) ## get .tar.gz file from site (url specified above)\n",
    "    with tarfile.open(fileobj=BytesIO(tar_gz.content)) as xmls: ## decompress tar.gz file, extract metadata\n",
    "        for xml in xmls.getnames()[0:100]: ## first 100 xmls\n",
    "            xmls.extract(xml) ## locally save single xml\n",
    "            with open(xml, \"rb\") as f: ## read contents of locally saved xml\n",
    "                Bs_data = BeautifulSoup(f, \"xml\") ## save xml contents into beautifulsoup\n",
    "                parsed_article_data = parse_xml(Bs_data) ## parse xml article metadata\n",
    "                article_data.append(parsed_article_data)\n",
    "            os.remove(xml) ## remove file from local instance\n",
    "            os.rmdir(str.split(xml, '/')[0]) ## remove created folder from local instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0933899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmc</th>\n",
       "      <th>article_title</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>epub_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMC176545</td>\n",
       "      <td>The Transcriptome of the Intraerythrocytic Dev...</td>\n",
       "      <td>PLoS Biology</td>\n",
       "      <td>8/18/2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMC176546</td>\n",
       "      <td>DNA Analysis Indicates That Asian Elephants Ar...</td>\n",
       "      <td>PLoS Biology</td>\n",
       "      <td>8/18/2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PMC176547</td>\n",
       "      <td>Borneo Elephants: A High Priority for Conserva...</td>\n",
       "      <td>PLoS Biology</td>\n",
       "      <td>8/18/2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PMC176548</td>\n",
       "      <td>Monitoring Malaria: Genomic Activity of the Pa...</td>\n",
       "      <td>PLoS Biology</td>\n",
       "      <td>8/18/2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PMC193604</td>\n",
       "      <td>\\nDrosophila Free-Running Rhythms Require Inte...</td>\n",
       "      <td>PLoS Biology</td>\n",
       "      <td>9/15/2003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pmc                                      article_title journal_title  \\\n",
       "0  PMC176545  The Transcriptome of the Intraerythrocytic Dev...  PLoS Biology   \n",
       "1  PMC176546  DNA Analysis Indicates That Asian Elephants Ar...  PLoS Biology   \n",
       "2  PMC176547  Borneo Elephants: A High Priority for Conserva...  PLoS Biology   \n",
       "3  PMC176548  Monitoring Malaria: Genomic Activity of the Pa...  PLoS Biology   \n",
       "4  PMC193604  \\nDrosophila Free-Running Rhythms Require Inte...  PLoS Biology   \n",
       "\n",
       "   epub_date  \n",
       "0  8/18/2003  \n",
       "1  8/18/2003  \n",
       "2  8/18/2003  \n",
       "3  8/18/2003  \n",
       "4  9/15/2003  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_metadata_df = pd.DataFrame(article_data)\n",
    "article_metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3def841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pmc              0\n",
       "article_title    0\n",
       "journal_title    0\n",
       "epub_date        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_metadata_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08a6eed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6800e",
   "metadata": {},
   "source": [
    "The desired metadata from the 100 articles were extracted successfully, and finish the process with no local storage being used.\n",
    "\n",
    "With respect to PubMed data, there is much more metadata which could be extracted here. There are roughly 36 million XMLs to extract, thus a rich data extraction can be performed using this notebook as a backbone.\n",
    "\n",
    "This exercise represents a basic ETL process. We extracted XMLs from a public domain, and transformed them into a dataframe containing article metatata. The final step would be loading the data to a storage server. Since this is a demo, we'll just keep this an ET process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
